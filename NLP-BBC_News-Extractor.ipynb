{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Mod5_NLP_Template.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kV2yn3KLU68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Do not chance these dependencies\n",
        "try:\n",
        "    import pytest\n",
        "except: !pip install pytest\n",
        "    import pytest\n",
        "# Import your dependencies here\n",
        "try: from bs4 import BeautifulSoup\n",
        "except: !pip install bs4\n",
        "    from bs4 import BeautifulSoup\n",
        "\n",
        "try: import spacy\n",
        "except: !pip install spacy\n",
        "    import spacy\n",
        "import requests, json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQF0kpOr-M_M",
        "colab_type": "text"
      },
      "source": [
        "*I made a function that scrapes the BBC News homepage and takes any links that are tageted at another /news page.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKgl-gPfCovN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gets all links from bbc news home page if they link to another /news story. \n",
        "def get_all_links():\n",
        "    full_url = 'https://www.bbc.co.uk/news'\n",
        "    get_links = requests.get(full_url).text\n",
        "    full_soup = BeautifulSoup(get_links, 'html.parser')\n",
        "\n",
        "    urls = []\n",
        "    for a in full_soup.find_all('a', class_ = 'gs-c-promo-heading', href=True):\n",
        "        if '/news' in a['href'] and 'https://' not in a['href']:\n",
        "            urls.append('https://www.bbc.co.uk' + a['href'])\n",
        "    urls = list(dict.fromkeys(urls))\n",
        "    \n",
        "    for url in urls:\n",
        "        article = requests.get(url).text\n",
        "        soup = BeautifulSoup(article, 'html.parser')\n",
        "        title = soup.title.text\n",
        "        print(urls.index(url), title, url)        \n",
        "    return urls, full_soup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "odEJJSb8-1V8",
        "colab_type": "text"
      },
      "source": [
        "*The links are output with enumeration so you may choose if there's a particular story you would like to run through the scraper.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "szVS-lBHDWlL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        },
        "outputId": "1b2cac14-e421-471d-801b-3d4a9e3bc8a1"
      },
      "source": [
        "# prints with leading list index so you can use that if you want to query just one in the full scrape.\n",
        "urls, full_soup = get_all_links()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx3ARhwl-hYX",
        "colab_type": "text"
      },
      "source": [
        "*Then, you may choose to supply the BBC scraper with one URL or many from the list created by the homepage scraper.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "auv9_6KdK8HS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bbc_scraper(url):\n",
        "    article = requests.get(url).text\n",
        "    soup = BeautifulSoup(article, 'html.parser')\n",
        "    content = soup.find(property='articleBody')\n",
        "    try: # simple error handling. It will fail if link leads to page which is not a text article as the searched for tags will not exist, eg video\n",
        "        content = [content.text for content in content.find_all('p')]\n",
        "        content = ' '.join(content)\n",
        "        title = soup.find(class_='story-body__h1').text\n",
        "        date = soup.find('ul', class_='mini-info-list').text.strip()\n",
        "\n",
        "\n",
        "    except:\n",
        "        title = soup.title.text\n",
        "        content = ' '\n",
        "        date = ' '\n",
        "        print('Invalid webpage for text analysis: ' + title + ' ' + url)\n",
        "        pass\n",
        "    \n",
        "    results_json = {'URL':url, 'Title':title, 'Date_published':date, 'Content':content}\n",
        "    results_json = json.dumps(results_json)\n",
        "    return results_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Bth-ZU-r8Y",
        "colab_type": "text"
      },
      "source": [
        "*If any of the URLs in the list created from scraping the homepage are not valid text based news stories then you will get a warning.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dD5TjwOxtgsQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "e597759d-253a-45b6-b0ad-644658d16f02"
      },
      "source": [
        "# to present all json objects for each url\n",
        "for url in urls:\n",
        "    print(bbc_scraper(url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLWuSGPXBkLf",
        "colab_type": "text"
      },
      "source": [
        "*The following three cells allow you to choose you query method*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIOfzkUOyMqT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will pull the content element for any story in the list generated, sliced on the enumerated value.\n",
        "string = json.loads(bbc_scraper(urls[5]))['Content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVJ2EtUGZ5pd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This will allow you to query a URL not in the list, so say one that is no longer on the News homepage\n",
        "url = 'https://www.bbc.co.uk/news/election-us-2020-53876958'\n",
        "string = json.loads(bbc_scraper(url))['Content']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIMy9r2Ddl6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To take all of the outputed JSON object rather than just the content element\n",
        "url = 'https://www.bbc.co.uk/news/election-us-2020-53876958'\n",
        "string = bbc_scraper(url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqT7yaNkBm1r",
        "colab_type": "text"
      },
      "source": [
        "*Import the Space module required for entity extraction*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NyV6hi7xqHc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import spacy language model\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FufkMP76Bs85",
        "colab_type": "text"
      },
      "source": [
        "*Function to extract entities from the string created above, by which ever method you chose.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDvLnOzZK79s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_entities(string):\n",
        "    people = []\n",
        "    places = []\n",
        "    organisations = []\n",
        "    doc = nlp(string)\n",
        "    for token in doc.ents:\n",
        "        if token.label_ == 'PERSON':\n",
        "            people.append(str(token)) #all added as strings to enable easier manipulation later\n",
        "        if token.label_ == 'GPE' or token.label_ == 'FAC': \n",
        "            places.append(str(token))\n",
        "        if token.label_ == 'ORG':\n",
        "            organisations.append(str(token))\n",
        "\n",
        "    # deduping the lists to remove repeatedly mentioned entities in the article\n",
        "    people = list(dict.fromkeys(people))\n",
        "    places = list(dict.fromkeys(places))\n",
        "    organisations = list(dict.fromkeys(organisations))\n",
        "\n",
        "    # create the json output\n",
        "    ent_out = {'people':people,'places':places,'organisations':organisations}\n",
        "    entities_json = json.dumps(ent_out)\n",
        "    #entities_json = json.loads(ent_out)\n",
        "    return entities_json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mGkHs5ciTFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "outputId": "01fd5ea1-e806-4385-bef7-4d931265b86e"
      },
      "source": [
        "json.loads(extract_entities(string))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iuiH-IhIK73H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "####################################################################\n",
        "# Test cases\n",
        "\n",
        "def test_bbc_scrape():\n",
        "    results = {'URL': 'https://www.bbc.co.uk/news/uk-52255054',\n",
        "                'Title': 'Coronavirus: \\'We need Easter as much as ever,\\' says the Queen',\n",
        "                'Date_published': '11 April 2020',\n",
        "                'Content': '\"Coronavirus will not overcome us,\" the Queen has said, in an Easter message to the nation. While celebrations would be different for many this year, she said: \"We need Easter as much as ever.\" Referencing the tradition of lighting candles to mark the occasion, she said: \"As dark as death can be - particularly for those suffering with grief - light and life are greater.\" It comes as the number of coronavirus deaths in UK hospitals reached 9,875. Speaking from Windsor Castle, the Queen said many religions had festivals celebrating light overcoming darkness, which often featured the lighting of candles. She said: \"They seem to speak to every culture, and appeal to people of all faiths, and of none. \"They are lit on birthday cakes and to mark family anniversaries, when we gather happily around a source of light. It unites us.\" The monarch, who is head of the Church of England, said: \"As darkness falls on the Saturday before Easter Day, many Christians would normally light candles together.  \"In church, one light would pass to another, spreading slowly and then more rapidly as more candles are lit. It\\'s a way of showing how the good news of Christ\\'s resurrection has been passed on from the first Easter by every generation until now.\" As far as we know, this is the first time the Queen has released an Easter message. And coming as it does less than a week since the televised broadcast to the nation, it underlines the gravity of the situation as it is regarded by the monarch. It serves two purposes really; it is underlining the government\\'s public safety message, acknowledging Easter will be difficult for us but by keeping apart we keep others safe, and the broader Christian message of hope and reassurance.  We know how important her Christian faith is, and coming on the eve of Easter Sunday, it is clearly a significant time for people of all faiths, but particularly Christian faith. She said the discovery of the risen Christ on the first Easter Day gave his followers new hope and fresh purpose, adding that we could all take heart from this.  Wishing everyone of all faiths and denominations a blessed Easter, she said: \"May the living flame of the Easter hope be a steady guide as we face the future.\" The Queen, 93, recorded the audio message in the White Drawing Room at Windsor Castle, with one sound engineer in the next room.  The Palace described it as \"Her Majesty\\'s contribution to those who are celebrating Easter privately\".  It follows a speech on Sunday, in which the monarch delivered a rallying message to the nation. In it, she said the UK \"will succeed\" in its fight against the coronavirus pandemic, thanked people for following government rules about staying at home and praised those \"coming together to help others\". She also thanked key workers, saying \"every hour\" of work \"brings us closer to a return to more normal times\".'}\n",
        "    scraper_result = bbc_scraper('https://www.bbc.co.uk/news/uk-52255054')\n",
        "    assert json.loads(scraper_result) == results\n",
        "\n",
        "\n",
        "def test_extract_entities_amazon_org():\n",
        "    input_string = \"I work for Amazon.\"\n",
        "    results_dict = {'people':[],\n",
        "                    'places':[],\n",
        "                    'organisations': ['Amazon']\n",
        "                    }\n",
        "    extracted_entities_results = extract_entities(input_string)\n",
        "    assert json.loads(extracted_entities_results) == results_dict\n",
        "\n",
        "\n",
        "def test_extract_entities_name():\n",
        "    input_string = \"My name is Bob\"\n",
        "    results_dict = {'people':['Bob'],\n",
        "                    'places':[],\n",
        "                    'organisations': []\n",
        "                    }\n",
        "    extracted_entities_results = extract_entities(input_string)\n",
        "    assert json.loads(extracted_entities_results) == results_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-EtNG-mJK7pt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_bbc_scrape()\n",
        "test_extract_entities_amazon_org()\n",
        "test_extract_entities_name()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}